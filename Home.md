


   欢迎您关注Ansj中文分词.分词的目的是创建一个高稳定可用的中文分词工具,可以利用到各种需要文字处理的场景中下面简单介绍一下Ansj中文分词的主要算法及特点.

### 数据结构

1. 高度优化Trie树
    
    在用户自定义词典以及各种类似于Map的场景中,大量使用的一个工具,众所周知,Trie具有高速的文本扫描能力,和较低的内存占用率,是最好的AC机之一,弦外之音,在我的认知范围内,貌似没有之一.相比其它结构在性能和构造上做到了很好的平衡,但是在java中,大量构建map尤其是hashmap,是一个非常昂贵的操作,通过对于一个map放入大量的key也注定其在自动拆箱装箱,以及解决冲突,大量hash匹配上做了过多的小消耗,虽然多数人认为,这种消耗属于纳秒级别的,但是对于动不动就上GB的文本来说,这个消耗是不可忽略的,所以作者在这里使用了首字母hash次字二分的方式来避免过多的消耗内存,也正应为有了这个机制.可以保证Ansj加载更多的用户自定义词典,有人问我具体的数字.大约500万词,1Gde 内存.在这里作者强烈推荐这个小家伙,你可以通过nlp-lang包来获取这个小工具,居家神器.

2. 三数组trie树
    
    三数组trie树,好吧我知道你们会吐槽我明明用的DAT(Double Array Tree),为什么在这里改成了TAT(Tree Array Tree)。我也不想如此，但是为了严谨一些真实的还原算法，的确是用了三个数组来实现的DAT的，主要是为了在判断词语之后避免一次无谓的倒退，算是空间换取时间的一个策略吧，具体感兴趣的可以参见nlp-lang中DAT的创建。对于DAT算法，个人觉得。如非必要，勿用，其在构造和修改上具有很多不确定性，不符合简单可依赖的理念，有兴趣的人可以了解下。在我小时候的博客中写了几篇关于DAT的文章，虽然很水。据说也有几个人看明白了。


### 机器学习

* `隐马尔科夫` `语言模型` `最短路径`    ansj中和ngram一起使用.通过两个词语之间的关联来确定,用来做语义消歧.
    





* `TF/IDF  词袋模型` **关键词抽**取中用到.用来确定一个词的重要程度.同时利用关键词对文章进行**自动摘要**




* `CRF` `类似CRF的上下文基于词的标注` 用来实现**新词发现**功能,同时新词发现也服务于关键词抽取



    

